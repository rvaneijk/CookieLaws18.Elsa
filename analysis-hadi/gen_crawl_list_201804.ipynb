{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to generate the new EU-Cookies project crawl list. \n",
    "# version 20180421 (based on version 2017-12-23 / 2017-10-24 / earlier)\n",
    "# The new crawl list is (1) for approx 20 countries (>2.5m BB subs)  (was 30 earlier)\n",
    "#                       (2) is chosen based on top sites per TLD \n",
    "#                       (3) as ranked by the Majestic million list (before: alexa, Cisco Umbrella) \n",
    "\n",
    "# output on: 2018-04-21\n",
    "\n",
    "# update: 2018-05-24: if tld-extract flags it as TLD, don't use it. won't resolve. majestic error.\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import tldextract\n",
    "import json\n",
    "from six.moves.urllib.parse import urlparse\n",
    "import re\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 => 20 .. 15\n"
     ]
    }
   ],
   "source": [
    "countries = pd.read_excel('../indata/Cookie-Countries-201804.xlsx', index_col='CC')\n",
    "n0 = len(countries)\n",
    "countries = countries[countries['In-Study-18']==1]\n",
    "n2 = len(countries[countries.EU_DP_status.isin(['EU','EEA'])&(countries.Broadband_2016>=1000000)]) \n",
    "print(n0, '=>', len(countries), '..', n2)  # 5 non-eu ; really small already removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majestic domains: 1000000\n",
      "Reached 400 for COM on rank #677 site prestashop.com\n",
      "Reached 400 for ORG on rank #3622 site dreamwidth.org\n",
      "Reached 200 for UK on rank #7141 site rmg.co.uk\n",
      "Reached 200 for DE on rank #8861 site kicker.de\n",
      "Reached 200 for JP on rank #12427 site skr.jp\n",
      "Reached 200 for IT on rank #17465 site unical.it\n",
      "Reached 200 for FR on rank #18057 site beteavone.fr\n",
      "Reached 200 for AU on rank #23546 site alphalink.com.au\n",
      "Reached 200 for CA on rank #25247 site mta.ca\n",
      "Reached 200 for ES on rank #26210 site anadelgado.es\n",
      "Reached 200 for PL on rank #28290 site blogx.pl\n",
      "Reached 200 for NL on rank #34914 site koninklijkhuis.nl\n",
      "Reached 200 for US on rank #43469 site campl.us\n",
      "Reached 200 for SE on rank #67598 site yogamamas.se\n",
      "Reached 200 for CH on rank #71868 site cash.ch\n",
      "Reached 200 for CZ on rank #80357 site army.cz\n",
      "Reached 200 for AT on rank #83100 site fjsoft.at\n",
      "Reached 200 for GR on rank #84511 site viva.gr\n",
      "Reached 200 for BE on rank #84844 site privacycommission.be\n",
      "Reached 200 for RO on rank #88518 site mihor.ro\n",
      "Reached 200 for HU on rank #96186 site merck-laboreszkoz.hu\n",
      "Reached 200 for PT on rank #115219 site profundamente.pt\n",
      "200000\n",
      "400000\n",
      "600000\n",
      "800000\n",
      "1000000\n",
      "skipped:  190 \n",
      "\n",
      " ['gc.ca', 'qc.ca', 'on.ca', 'gouv.fr', 'bc.ca', 'waw.pl', 'asso.fr', 'vic.gov.au', 'tx.us', 'ca.us', 'info.pl', 'mn.us', 'ny.us', 'fl.us', 'gv.at', 'pa.us', 'nj.us', 'ab.ca', 'or.at', 'gob.es', 'il.us', 'qld.gov.au', 'or.us', 'fed.us', 'wroclaw.pl', 'wa.gov.au', 'va.us', 'oh.us', 'mb.ca', 'md.us', 'nc.us', 'wi.us', 'biz.pl', 'szczecin.pl', 'ns.ca', 'ma.us', 'warszawa.pl', 'mi.us', 'tokyo.jp', 'opole.pl', 'ga.us', 'sa.gov.au', 'in.us', 'katowice.pl', 'ms.us', 'ak.us', 'wa.us', 'sk.ca', 'olsztyn.pl', 'rzeszow.pl', 'vic.edu.au', 'nsw.edu.au', 'ia.us', 'la.us', 'bydgoszcz.pl', 'nm.us', 'radom.pl', 'az.us', 'tm.fr', 'al.us', 'zgora.pl', 'wy.us', 'mo.us', 'czest.pl', 'qld.edu.au', 'nv.us', 'tn.us', 'bialystok.pl', 'sklep.pl', 'pe.ca', 'nh.us', 'tas.gov.au', 'vt.us', 'ok.us', 'jgora.pl', 'roma.it', 'kalisz.pl', 'fi.it', 'ra.it', 'mi.it', 'ct.us', 'bz.it', 'ar.us', 'sc.us', 'cci.fr', 'lubin.pl', 'milano.it', 'pila.pl', 'marche.it', 'me.us', 'wa.edu.au', 'sd.us', 'kanagawa.jp', 'wv.us', 'ne.us', 'osaka.jp', 'firenze.it', 'rybnik.pl', 'ri.us', 'ut.us', 'kyoto.jp', 'toscana.it', 'nb.ca', 'aichi.jp', 'shizuoka.jp', 'lombardia.it', 'to.it', 'yk.ca', 'hi.us', 'nf.ca', 'hokkaido.jp', 'de.us', 'aeroport.fr', 'ky.us', 'hiroshima.jp', 'ks.us', 'elblag.pl', 'konin.pl', 'tychy.pl', 'pisz.pl', 'trieste.it', 'id.us', 'tn.it', 'sa.edu.au', 'mt.us', 'chiba.jp', 'nl.ca', 'media.pl', 'hyogo.jp', 'torino.it', 'venezia.it', 'bo.it', 'nieruchomosci.pl', 'nagano.jp', 'malopolska.pl', 'nd.us', 'auto.pl', 'beskidy.pl', 'info.ro', 'augustow.pl', 'nt.ca', 'veneto.it', 'oz.au', 'pulawy.pl', 'cieszyn.pl', 'mazowsze.pl', 'slupsk.pl', 'piemonte.it', 'bieszczady.pl', 'fvg.it', 'na.it', 'suwalki.pl', 'genova.it', 're.it', 'elk.pl', 'gniezno.pl', 'bologna.it', 'walbrzych.pl', 'emilia-romagna.it', 'sosnowiec.pl', 'napoli.it', 'polkowice.pl', 'nysa.pl', 'bialowieza.pl', 'bielawa.pl', 'karpacz.pl', 'malbork.pl', 'bytom.pl', 'bedzin.pl', 'pomorze.pl', 'legnica.pl', 'mazury.pl', 'grajewo.pl', 'swidnica.pl', 'jaworzno.pl', 'forum.hu', 'nom.es', 'boleslawiec.pl', 'lukow.pl', 'wloclawek.pl', 'k12.co.us', 'mielec.pl', 'pc.pl', 'lomza.pl', 'pp.se', 'a.se', 'olawa.pl', 'info.hu', 'dni.us', 'nsn.us'] \n",
      "\n",
      "\n",
      "=> selected domains 4800\n"
     ]
    }
   ],
   "source": [
    "top1m = pd.read_csv('../indata/majestic_million-20180421.csv', index_col='GlobalRank')\n",
    "\n",
    "df = pd.DataFrame(columns=['tld', 'global_rank', 'rank', 'domain', 'cat'])\n",
    "\n",
    "\n",
    "# for simplicity just .com/.org now\n",
    "# - top tlds: .com, .org,! (plus .net, .io, ru, .br, .cn., as well as .tv .co .me, lastly .gov, .edu, .mobi, .biz...)\n",
    "# - .NET make's lettile sense -- often not user-facing or have .com alts. \n",
    "# - beheavior of others might be too specfic\n",
    "interesting_ggtlds = ('COM', 'ORG')\n",
    "\n",
    "# collecting 200, although i'll be using 100\n",
    "cctld_limit = 200\n",
    "tld_collected = {}\n",
    "skipped = []\n",
    "# btw: with current config, a bunch countries had 199 at most\n",
    "# stats from before (ubmbrella list)                \n",
    "# - interesting, these small countries were at the end... ('EE': 76, 'LV': 88, 'SI': 78)\n",
    "# - (I thought of removing re unbalanced and two no CG VPNs. but rob thought slovenia rules interesting)\n",
    "\n",
    "\n",
    "print('majestic domains:', len(top1m))\n",
    "\n",
    "for ix, row in top1m.iterrows():\n",
    "    if ix % 200000 == 0:\n",
    "        print(ix)\n",
    "        \n",
    "    tld, tld_rank, domain = row['TLD'].upper(), row['TldRank'], row['Domain']    \n",
    "    if tld in countries.index or tld in interesting_ggtlds:\n",
    "        limit = cctld_limit  if tld not in interesting_ggtlds else 2 * cctld_limit\n",
    "        if len(tld_collected.setdefault(tld, [])) < limit:\n",
    "            # majestic's full domain anmes are often better than what 'tldextract' returns\n",
    "            # - e.g. plus.google.com vs google.com\n",
    "            # - in four cases I'd say 'tldextract' is even wrong: gc.ca, waw.pl, ...\n",
    "            # - in a few cases (e.g. blogspot) it is a question mark. but let's be consistent\n",
    "            # - (if row.Domain != : print...)\n",
    "            \n",
    "            # if domain == 'gc.ca' or domain == 'waw.pl': raise\n",
    "                \n",
    "            if not tldextract.extract(domain).registered_domain:\n",
    "                #print('skipping ', domain)\n",
    "                skipped.append(domain)\n",
    "                continue\n",
    "            \n",
    "            assert not any(df.domain == domain)  # sanity check no duplicates            \n",
    "            df = df.append({'tld':tld, 'global_rank':ix, 'rank':tld_rank, 'domain':domain}, ignore_index=True)\n",
    "            tld_collected[tld].append(domain)\n",
    "            if len(tld_collected[tld]) == limit:\n",
    "                print('Reached %d for %s on rank #%d site %s' % (limit, tld, ix, domain))\n",
    "\n",
    "         \n",
    "print('skipped: ', len(skipped), '\\n\\n', skipped, '\\n\\n')        \n",
    "        \n",
    "for t in set(df.tld):\n",
    "    l = len(df[df.tld==t])  \n",
    "    if l < cctld_limit: \n",
    "        print('not reached', t, l)\n",
    "        \n",
    "print('=> selected domains', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tldextract.extract(domain).registered_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.tld=='SE']\n",
    "#df2 = df.copy()\n",
    "#ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('zackw urls:', 330358)\n"
     ]
    }
   ],
   "source": [
    "# let's load zack-w's cateogrization data\n",
    "zj = json.load(open('../indata/zackw_urls_with_topics_20160223_data.json'))\n",
    "zj = zj['urls']   # keys: 'clab_categories', 'sources', 'urls'\n",
    "zu = {}\n",
    "#dbg_map = {}\n",
    "for u in zj:\n",
    "    uu = urlparse(u).netloc.lower()  # get domain\n",
    "    uu = re.sub('^www.', '', uu)  # remove starting www.\n",
    "    # tldextract.extract(uu).registered_domain  # no: let's no confuse google.nl and books.google.nl        \n",
    "    # if uu.endswith('google.nl'): DBG_google.add(uu)    \n",
    "    #dbg_map[u] = uu\n",
    "    l = zu.setdefault(uu, list())\n",
    "    for dt, cats in zj[u]['access_results_usa'].items():\n",
    "        l.append(tuple(cats))\n",
    "print('zackw urls:', len(zu))\n",
    "\n",
    "cnt_cat = Counter()\n",
    "for u,l in zu.items():\n",
    "    if type(l[0]) is tuple:\n",
    "        top = Counter()\n",
    "        for cats in l:\n",
    "            for c in cats:\n",
    "                top[c] += 1\n",
    "        top = sorted(top.items(), key=lambda x:x[1], reverse=True)  # sort by category count\n",
    "        for top1 in top:\n",
    "            if not top1[0].startswith('error:') and not top1[0].startswith('junk:'):\n",
    "                # get first, except if it's error/junk (error: 403, 404, 503, TLS error, crawler failure, ...)\n",
    "                break  \n",
    "        l.insert(0, top1[0])\n",
    "        l.insert(1, top)\n",
    "        cnt_cat[top1[0].split(':')[0]] += 1         \n",
    "# pprint(cnt_cat)  # huh, not the best. 24k junk, 50k error; 51k porn :)\n",
    "# for d in DBG_google: print('\\n', '********', d, '********', '\\n', zu[d])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BE', 27.0, '%')\n",
      "('FR', 66.0, '%')\n",
      "('DE', 75.0, '%')\n",
      "('JP', 80.0, '%')\n",
      "('HU', 28.0, '%')\n",
      "('COM', 95.0, '%')\n",
      "('NL', 37.0, '%')\n",
      "('PT', 19.0, '%')\n",
      "('RO', 23.0, '%')\n",
      "('PL', 34.0, '%')\n",
      "('CH', 35.0, '%')\n",
      "('GR', 22.0, '%')\n",
      "('CA', 46.0, '%')\n",
      "('IT', 41.0, '%')\n",
      "('CZ', 31.0, '%')\n",
      "('AU', 59.0, '%')\n",
      "('AT', 28.0, '%')\n",
      "('ES', 53.0, '%')\n",
      "('US', 16.0, '%')\n",
      "('UK', 69.0, '%')\n",
      "('ORG', 80.0, '%')\n",
      "('SE', 30.0, '%')\n",
      "(10, 84.0, '%')\n",
      "(30, 73.0, '%')\n",
      "(50, 68.0, '%')\n",
      "(70, 65.0, '%')\n",
      "(90, 61.0, '%')\n"
     ]
    }
   ],
   "source": [
    "# now combine zack with our data\n",
    "\n",
    "# now website type is sth else and might force me to use alexa-top-1m\n",
    "for ix in df.index:\n",
    "    s, tld = df.loc[ix, 'domain'], df.loc[ix, 'tld']    \n",
    "    if s in zu:\n",
    "        # if tld=='NL':\n",
    "        #    pprint(s, zu[s][1])\n",
    "        # so news + education are fine; \n",
    "        # but: travel under entertainment? commerce subs? hosting under sw? esp. platforms (e.g. isps) can be bad\n",
    "        df.loc[ix, 'cat'] = zu[s][0].split(':')[0]\n",
    "\n",
    "# debug code:\n",
    "# for d in ['netflix.com', 'instagram.com', 'microsoft.com', 'hola.org', 'amazonaws.com', 'office365.com', 'w.org',\n",
    "#           'geenstijl.nl', 'gettyimages.nl', 'volkskrant.nl', 'daskapital.nl']:\n",
    "#     print('*****', d, '*****')  \n",
    "#     for u, uu in dbg_map.items():\n",
    "#          if uu == d:\n",
    "#              pprint(zj[u])\n",
    "#     top, top_p = Counter(), Counter()\n",
    "#     for cats in zu[d][1:]:\n",
    "#         for c in cats:\n",
    "#             top[c] += 1\n",
    "#             top_p[c.split(':')[0]] += 1\n",
    "#     top = sorted(top.items(), key=lambda x:x[1], reverse=True)  \n",
    "#     top_p = sorted(top_p.items(), key=lambda x:x[1], reverse=True)  \n",
    "#     pprint(top, top_p)\n",
    "        \n",
    "# STATS: I guess this is ok and what we have. more than half has no TLD\n",
    "for tld in set(df.tld):\n",
    "    n = len(df[df.tld==tld])\n",
    "    n1 = len(df[(df.tld==tld)&~df.cat.isnull()])\n",
    "    if n1 / n < 0.25 or len(tld) == 3:\n",
    "        print(tld, round(n1*100/n), '%')                \n",
    "        # lowest: SI 13, LV 16, HR 18, but also a few 20-30. majority under 50%. org & com around 50%.\n",
    "for r in range(10, 101, 20):\n",
    "    n = len(df[df['rank']<=r])\n",
    "    n1 = len(df[(df['rank']<=r)&(~df.cat.isnull())])\n",
    "    print(r, round(n1*100/n), '%')  # so after 10, or 100, it's always about 40%\n",
    "\n",
    "# CONCLUSIONS:\n",
    "#  - categories generally ok; cuont with weights unnecssary; \n",
    "#  - TODO: major problem is when two top ones have same count; (get two?)\n",
    "#  - TODO: i should check his paper to see what's up with the classification order, how authors use it\n",
    "#  - (re overlap) let's just use this TLD list; the ones that are empty, are empty. sth to deal with later        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 4800\n",
      "         global_rank        domain  cat\n",
      "tld rank                               \n",
      "AT  3           1435  univie.ac.at  NaN\n",
      "    4           1438     kriesi.at  NaN\n",
      "    5           2231  tuwien.ac.at  NaN\n",
      "    6           2259        orf.at  NaN\n",
      "    7           2332     google.at  NaN\n"
     ]
    }
   ],
   "source": [
    "# export!\n",
    "dft = df.set_index(['tld', 'rank']).sort_index()\n",
    "print(len(set(df.tld)), len(df))\n",
    "print(dft.head())\n",
    "dft.to_csv('../indata/scanlist_20180423b-tmp.csv')\n",
    "\n",
    "# TODO: this list is mapped SOOO wrongly that I wonder if I am mapping/loading wrongly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's overlap with old list non loadables\n",
    "df_ping = pd.read_csv(\"../indata/scanlist_20180421-ping.csv\", index_col=['tld', 'rank'])\n",
    "df_ping = df_ping.join(dft, lsuffix='__p')\n",
    "\n",
    "df_ping[~df_ping.ping_ip.isnull()&df_ping.domain.isnull()]  \n",
    "df_ping.reset_index(inplace=True)\n",
    "    # great, so indeed majority of ping/look fails are TLD extraction erros by majestic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dft.head()\n",
    "# btw, how many in large scan 20180422 expected to be errors?\n",
    "\n",
    "lim = [\"CH\",\"BE\",\"RO\",\"PL\",\"NL\",\"CA\",\"ES\",\"IT\",\"FR\",\"DE\",\"US\",\"UK\",\"COM\"]\n",
    "print(len(df_ping[df_ping.tld.isin(lim)&df_ping.ping_ip.isnull()&(df_ping['rank']<=50)]))  # 111\n",
    "\n",
    "# [hadi@econsec06 CookieLaws18]$ grep Finished nohup.out*\n",
    "# nohup.out.be: Finished crawl (time: 19006s)  > 128 ERROR\n",
    "# nohup.out.ch: Finished crawl (time: 20605s) > 126 ERROR\n",
    "# nohup.out.de: Finished crawl (time: 19912s) > 124 ERROR\n",
    "# nohup.out.fr: Finished crawl (time: 18631s) > 124 ERROR\n",
    "# nohup.out.nl: Finished crawl (time: 18680s) > 125 ERROR\n",
    "# nohup.out.ro: Finished crawl (time: 19441s) > 128 ERROR\n",
    "# nohup.out.us: Finished crawl (time: 20462s) > 118 ERROR\n",
    "\n",
    "# nohup.out.ca: Finished crawl (time: 19523s) > 122 ERROR\n",
    "# nohup.out.es: Finished crawl (time: 19871s) > 125 ERROR\n",
    "# nohup.out.it: Finished crawl (time: 19552s) > 123 ERROR\n",
    "# nohup.out.pl: Finished crawl (time: 19404s) > 127 ERROR\n",
    "# nohup.out.uk: Finished crawl (time: 13501s) >> TWO CPUS >> 154 ERROR >> clearly more ! :/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
