{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to give stats about a crawl (quickly)\n",
    "# Author Hadi, 25 May 2018\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "#import json\n",
    "#import re\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "# TODO: turn this into a script (with parts bellow)\n",
    "# TODO: below is good. we can scale up, I think. just to figure out that one error. if important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT_DIR = \"/home/hadi/Devel/CookieLaws18/out/crawl-vpnDE-20180524.2119-1/\"\n",
    "INPUT_DIR = \"/home/hadi/Devel/CookieLaws18/out/crawl-vpnUS-20180524.2124-1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS:  bool_success\n",
      "-1     16\n",
      " 0     34\n",
      " 1    508\n",
      "Name: site, dtype: int64\n",
      "HTTP CODE response_status\n",
      "200     52\n",
      "301    356\n",
      "302     84\n",
      "303      1\n",
      "307      3\n",
      "403      4\n",
      "404      3\n",
      "503      2\n",
      "Name: site, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "dbcon = sqlite3.connect(INPUT_DIR + \"crawl-data.sqlite\")\n",
    "\n",
    "# sqlite reference: https://sqlite.org/lang_corefunc.html\n",
    "sites = pd.read_sql_query(\"\"\"SELECT s.visit_id, \n",
    "                            site_url as site, \n",
    "                            bool_success, \n",
    "                            group_concat(response_status) as response_status, \n",
    "                            group_concat(location) as redirect, \n",
    "                            count(*) as response_count\n",
    "                          FROM site_visits s \n",
    "                          LEFT JOIN CrawlHistory h\n",
    "                               ON s.site_url=h.arguments AND h.command='GET'\n",
    "                          LEFT JOIN http_responses r\n",
    "                               ON s.visit_id=r.visit_id AND LOWER(RTRIM(s.site_url))=RTRIM(r.url, '/')\n",
    "                          GROUP BY s.visit_id\"\"\", \n",
    "                          dbcon)\n",
    "\n",
    "print(\"SUCCESS: \", sites.groupby('bool_success').count().site)\n",
    "\n",
    "# HTTP CODES.\n",
    "t = sites[(sites.response_count==1)&(sites.bool_success==1)].copy()\n",
    "print(\"HTTP CODE\", t.groupby(\"response_status\").count().site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL COOKIES 25857\n",
      "AVERAGE COOKIES 46.33870967741935\n"
     ]
    }
   ],
   "source": [
    "# COOKIES. \n",
    "cookies = pd.read_sql_query(\"\"\"SELECT site_url as site, host as chost, name, value, \n",
    "                                      is_session, change, expiry, creationTime\n",
    "                            FROM site_visits s \n",
    "                            LEFT JOIN javascript_cookies c \n",
    "                            ON s.visit_id=c.visit_id ORDER BY s.visit_id\"\"\", \n",
    "                            dbcon)\n",
    "\n",
    "\n",
    "print(\"TOTAL COOKIES\", cookies.count().is_session)\n",
    "print(\"AVERAGE COOKIES\", np.mean(cookies.groupby(\"site\").count().is_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL JS CALLS 230293\n",
      "AVERAGE JS CALLS 477.7863070539419\n"
     ]
    }
   ],
   "source": [
    "# JS CALLS\n",
    "js_calls = pd.read_sql_query(\"\"\"SELECT site_url as site, script_url as js_url, script_url as jhost\n",
    "                                FROM site_visits s LEFT JOIN javascript j\n",
    "                                ON s.visit_id=j.visit_id \n",
    "                                WHERE script_url is not null\n",
    "                                ORDER BY s.visit_id\"\"\", dbcon)\n",
    "\n",
    "print(\"TOTAL JS CALLS\", js_calls.count().js_url)\n",
    "print(\"AVERAGE JS CALLS\", np.mean(js_calls.groupby(\"site\").count().js_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screenshots:  0\n",
      "page sources:  0\n"
     ]
    }
   ],
   "source": [
    "# NUMBER SCREENSHOTS; NUMBER HTML PAGES\n",
    "from glob import glob\n",
    "print(\"screenshots: \", len(glob(INPUT_DIR + \"screenshots/*\")))\n",
    "print(\"page sources: \", len(glob(INPUT_DIR + \"sources/*\")))\n",
    "\n",
    "# TODO: could also get sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF NEEDED TO GET NUM TLDS IN VISITED\n",
    "# (Helper function to remove the http, https and extract domains from site_url)\n",
    "\n",
    "from tldextract import extract as tldextract  # pip install tldextract\n",
    "\n",
    "def strip_site(site):\n",
    "    \"\"\"Removes leading http:// or https:// and trailing '/'\"\"\"\n",
    "    if site is None:\n",
    "        return None\n",
    "    site = site.lower().strip()\n",
    "    if site.startswith('http'):\n",
    "        site = site.replace('https://', '').replace('http://', '')\n",
    "    if site.startswith('www.'):\n",
    "        site = site[4:]\n",
    "    if site.endswith('/'):\n",
    "        site = site[:-1]\n",
    "    return site\n",
    "\n",
    "def extract_domain(site):\n",
    "    \"\"\"Returns domain+tld from a full domain\"\"\"\n",
    "    site = strip_site(site)  # just to be sure\n",
    "    ext = tldextract(site)\n",
    "    ret = \".\".join([ss for ss in ext[-2:] if ss])\n",
    "    # In case of gov.ie, or IPs, our 'ret' works better than 'ext.registered_domain'\n",
    "    return ret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
